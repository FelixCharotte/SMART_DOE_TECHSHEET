<p align="center">
  <img src="app/frontend/logo.png" alt="Smart DOE Logo" width="200" />
</p>

# Smart DOE
A smart document management system that uses AI-powered analysis to automatically classify and organize documents within As-Built Documentation.

## Features

- **Intelligent Classification**:
  - LLM-based classification using Azure OpenAI
  - Semantic similarity via vector search (`pgvector`)
  - Confidence scoring and manual override interface

- **Advanced Document Analysis via Azure Document Intelligence**:
  - Parsing with *prebuilt* models (PDF, DOCX, XLSX, PNG, etc.)
  - Title block detection on 2D plans using the custom model `cartouche_plan_detection`
  - Information extraction from title blocks using the custom model `cartouche_information_extraction_upgrade`

- **Custom Folder Tree Generation**:
  - Input: CCTP or descriptive documents
  - Method: similarity search over embeddings to find DOE-relevant content
  - Output: folder tree structure generated by LLM

- **RAG Chatbot**:
  - Ask questions on uploaded/classified documents
  - Combines retrieval from vector database with LLM responses

- **Modular Architecture**:
  - **Backend**: FastAPI for processing and API access
  - **Frontend**: Streamlit for interactive UI

- **PostgreSQL with Vector Storage**:
  - Embeddings stored using `pgvector` extension for efficient semantic search


## Prerequisites

- Python 3.8+
- PostgreSQL with `pgvector` extension
- Access to:
  - Azure OpenAI
  - Azure Document Intelligence (prebuilt + custom models)

## Environment Variables

Create a `.env` file with the following variables:

```env
AZURE_OPENAI_ENDPOINT=your_azureopenai_endpoint
AZURE_OPENAI_API_KEY=your_azureopenai_key
AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT=your_azuredocumentintelligence_endpoint
AZURE_DOCUMENT_INTELLIGENCE_KEY=your_azuredocumentintelligence_key
POSTGRES_CONNECTION_STRING=postgresql://user:password@localhost:5432/database
```

## Installation

1. Clone the repository:
```bash
cd DOE_automation
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

3. Set up PostgreSQL with pgvector:
```sql
CREATE EXTENSION vector;
```

## Usage

1. Start the Streamlit application:
```bash
streamlit run app.py
```

2. Define folder structure
Use the integrated JSON editor to create or modify your custom folder hierarchy.

3. Upload documents
Drop in PDF, DOCX, XLSX, or image files for parsing, classification, and metadata extraction.

4. Explore and Manage
Navigate through your structured folders in the Visualization tab. Override classifications if needed.

5. Generate a folder tree from CCTP
Upload CCTP or descriptive documents and let the system:
- Search for relevant DOE content via semantic similarity
- Use LLM to generate a tailored folder hierarchy

6. Chat with your documents
Ask questions in natural language using the Chat RAG tab.
The system retrieves relevant chunks from your uploaded files and generates accurate responses.

## Project Structure

```
smart-doe/
├── backend/
│ └── src/ # Core functional modules
|   ├── database/ # Database models and schema
|   ├── document_processing/ # Document parsing, chunking, and metadata extraction
|   ├── folder_management/ # Folder manipulation and hierarchy logic
|   ├── tree_generation/ # Embedding search and LLM-based tree suggestion
|   ├── RAG/ # Retrieval-Augmented Generation (chatbot)
|   └── utils/ # Shared utilities (helpers, constants, etc.)
├── frontend/
│ ├── components/ # Streamlit UI components (cards, trees, etc.)
│ ├── pages/ # Streamlit page routes (classification, explorer, tree, etc.)
| ├── services/ # API service layer
| ├── app.py # FastAPI application entry point
│ ├── config.py / config.yaml # Configuration settings (env vars, model keys, etc.)
| ├── logo.png # Project logo used in UI
| └── authentication.py # Auth handling (to be expanded)
└── requirements.txt # Python dependencies
```

## Technical Details

### Classification Process

### 🧾 Step 1 — Document Parsing (via Azure Document Intelligence)

- **If 2D Plan:**
  - Title block detection using the `cartouche_plan_detection` custom model
  - Information extraction from the detected block using `cartouche_information_extraction_upgrade`

- **Otherwise:**
  - Attempt to extract the title block from the first page

- **Fallback:**
  - If no title block is detected, use the standard *prebuilt* model to parse document content

📤 **Output**: Chunked document text

---

### 🏷️ Step 2 — Metadata Extraction

Automatically retrieves document metadata such as:
- Title
- Type
- File weight
- Date  
---

### 🗳️ Step 3 — Classification by Majority Voting

Three independent classification runs:
1. On the **first 5 chunks**
2. On **5 random chunks**
3. On the **last 5 chunks**

🧠 **Confidence scoring based on majority:**
- ✅ **0.9** → All 3 votes agree
- ⚠️ **0.6** → 2 out of 3 agree
- ❌ **0.3** → No agreement → fallback to the result from the first group

🔧 Results are shown with confidence levels and can be manually edited in **Explorer** page.

---

### 🧬 Step 4 — Embedding Generation

- Document chunks are converted into vector embeddings using **Azure OpenAI**
- Stored in **PostgreSQL** with `pgvector` for semantic search and similarity-based operations

### Database Schema

The system uses a PostgreSQL database with the following main tables:
- `folders`: Stores the folder hierarchy
- `documents`: Stores document metadata and embeddings
- `document_classifications`: Tracks classification history
- `folder_structure_versions`: Manages structure versions informations
- `langchain_pg_collection`: Manages embedding collection
- `langchain_pg_embedding`: Store vector embedding for semantic search
- `structure_folder` : Links folders to their associated structure templates


